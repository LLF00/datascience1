{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'APTX9998'\n",
    "password = 'Acp123456'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r\"D:\\5.ACP\"\n",
    "output_filename = \"BOA.csv\"\n",
    "output = f\"{output_path}\\\\{output_filename}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.FirefoxOptions()\n",
    "options.add_argument('--headless')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_down(browser):\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_twitter_search():\n",
    "    with webdriver.Firefox(options=options) as browser:\n",
    "        url = 'https://twitter.com/BankofAmerica'\n",
    "        browser.get(url)\n",
    "\n",
    "        wait = WebDriverWait(browser, 10)\n",
    "\n",
    "        login_button = wait.until(EC.presence_of_element_located((By.XPATH, '//a[@href=\"/login\"]')))\n",
    "        login_button.click()\n",
    "\n",
    "        username_input = wait.until(EC.presence_of_element_located((By.XPATH, './/input[@name=\"text\"]')))\n",
    "        username_input.send_keys(username)\n",
    "        username_input.send_keys(Keys.RETURN)\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        password_input = wait.until(EC.presence_of_element_located((By.XPATH, './/input[@name=\"password\"]')))\n",
    "        password_input.send_keys(password)\n",
    "        password_input.send_keys(Keys.RETURN)\n",
    "\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, '//input[@enterkeyhint=\"search\"]')))\n",
    "\n",
    "        # Removing the topic search\n",
    "        search_input = browser.find_element(By.XPATH, '//input[@enterkeyhint=\"search\"]')\n",
    "        search_input.send_keys(Keys.RETURN)\n",
    "\n",
    "        current_tweets = 0\n",
    "        user_data = []\n",
    "        text_data = []\n",
    "        time_data = []\n",
    "\n",
    "        while True:\n",
    "            for _ in range(5):\n",
    "                scroll_down(browser)\n",
    "\n",
    "            tweets = wait.until(EC.presence_of_all_elements_located((By.XPATH, '//article[@role=\"article\"]')))\n",
    "\n",
    "            for tweet in tweets:\n",
    "                try:\n",
    "                    user = tweet.find_element(By.XPATH, './/span[contains(text(), \"@\")]').text\n",
    "                    text = tweet.find_element(By.XPATH, \".//div[@lang]\").text\n",
    "                    tweet_time = tweet.find_element(By.XPATH, \".//time\").get_attribute(\"datetime\")\n",
    "\n",
    "                    tweets_data = [user, text, tweet_time]\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting tweet: {e}\")\n",
    "                    tweets_data = ['user', 'text', \"time\"]\n",
    "\n",
    "                user_data.append(tweets_data[0])\n",
    "                text_data.append(\" \".join(tweets_data[1].split()))\n",
    "                time_data.append(tweets_data[2])\n",
    "\n",
    "                current_tweets += 1\n",
    "\n",
    "            print(f\"Scraped {current_tweets} tweets\")\n",
    "\n",
    "        df = pd.DataFrame({'user': user_data, 'text': text_data, 'time': time_data})\n",
    "        df.to_csv(output, index=False)\n",
    "        print(f\"Total {current_tweets} tweets scraped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_twitter_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 7 tweets\n",
      "Scraped 15 tweets\n",
      "Scraped 22 tweets\n",
      "Scraped 32 tweets\n",
      "Scraped 40 tweets\n",
      "Scraped 47 tweets\n",
      "Scraped 53 tweets\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "   main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
